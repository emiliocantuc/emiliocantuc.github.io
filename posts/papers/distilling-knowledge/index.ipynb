{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Distilling the Knowledge in a Neural Network\"\n",
    "subtitle: \"Revisiting and implementing part of the classical paper\"\n",
    "date: 2025-01-28\n",
    "categories: [deep learning, paper]\n",
    "format:\n",
    "  html:\n",
    "    other-links:\n",
    "      - text: Paper\n",
    "        icon: book\n",
    "        href: https://arxiv.org/abs/1503.02531\n",
    "    toc: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-summary: Imports and model evaluation function\n",
    "#| output: false\n",
    "#| echo: false\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os, functools, itertools, collections\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "for d in ['data', 'models', 'logs']: os.makedirs(d, exist_ok = True)\n",
    "DATA_DIR = '../data'\n",
    "\n",
    "device = torch.device(\n",
    "    'cuda' if torch.cuda.is_available() else\n",
    "    ('mps' if torch.backends.mps.is_available() else\n",
    "    'cpu')\n",
    ")\n",
    "\n",
    "def set_seed(seed = 42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if device.type == 'cuda':\n",
    "        print('Setting seed for CUDA')\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idea\n",
    "\n",
    "This classic paper introduced *distillation* as a way of transferring knowledge from a big network teacher into a small one. The core observation is that we should use the big model's output distribution as soft labels to train the small model. \n",
    "\n",
    "Remember that in classification we measure the [cross-entropy](https://en.wikipedia.org/wiki/Cross-entropy) loss, given the predicted $\\hat y_c$ and correct $y_c$ class probabilities of an example, with:\n",
    "\n",
    "$$\n",
    "L(\\hat y,y) = -\\sum_c y_c \\log \\hat y_c\n",
    "$$\n",
    "\n",
    "To use soft labels we just set $y = f_{\\text{big}}(x)$.\n",
    "\n",
    "These soft labels provide a much richer training signal for the smaller model, especially when the larger model distributes its probability mass across multiple classes (i.e. when the labels have high entropy). To force this high entropy, the authors propose increasing the temperature $T$ of the softmax layer in the larger model to produce the soft labels. The small model trains with this same temperature but then sets it to 1 during testing.\n",
    "\n",
    "![Increasing the temperature of the big model produces softer and more informative labels.](images/demo.png)\n",
    "\n",
    "They also had better results by adding a small term to the loss function with the regular hard-labeled cross-entropy. The reasoning is that the model may not have enough capacity to learn the soft targets, so *\"erring in the direction of the correct answer turns out to be helpful\"*. If we write the output of a model with temperature $T$ as $f(x; T)$, then the complete loss is\n",
    "\n",
    "$$\n",
    "L_{\\text{distill}}(x,y) = a T^2 \\cdot L\\left[ f_{\\text{small}}(x; T), f_{\\text{big}}(x; T) \\right] + (1-a) \\cdot L \\left [ f_{\\text{small}}(x; 1), y \\right ]\n",
    "$$\n",
    "\n",
    "The first term is scaled by $T^2$ because the magnitudes of the gradients scale as $T^{-2}$ and we want to control the contribution of each term by changing only $a$.\n",
    "\n",
    "::: {.callout-note collapse=\"true\" title=\"Why do the gradient magnitudes scale as $T^{-2}$?\" icon=false}\n",
    "\n",
    "Let $z$ be the logits, then the output $i$th entry of the softmax layer with temperature $T$ is\n",
    "$$\n",
    "\\sigma_T(z)_i = \\frac{e^{z_i/T}}{\\sum_j e^{z_j/T}} = \\hat y_i\n",
    "$$\n",
    "Plugging into the loss\n",
    "$$\n",
    "L(\\hat y, y) = -\\sum_i y_i \\log \\left(\\frac{e^{z_i/T}}{\\sum_j e^{z_j/T}} \\right) = -\\frac{1}{T}\\sum_i y_i z_i + (1) \\log \\left( \\sum_j e^{z_j/T} \\right)\n",
    "$$\n",
    "and differentiating w.r.t. $z_i$ (don't forget the chain rule), we get\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_i} = -\\frac{1}{T} y_i + \\frac{1}{ \\sum_j e^{z_j/T}} \\times e^{z_i/T} (1/T) = \\frac{1}{T}(\\sigma_T(z)_i - y_i)\n",
    "$$\n",
    "So, we see that\n",
    "$$\n",
    "|| \\nabla L||_2^2 = \\frac{1}{T^2} \\sum_i (\\sigma_T(z)_i - y_i)^2 \\propto L^{-2}\n",
    "$$\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST\n",
    "\n",
    "We try out distillation on the small-scale MNIST experiment that the authors describe. They use a two-layer linear ReLU architecture with dropout, a jitter image augmentation, and max norm as regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: Helper functions\n",
    "#| echo: false\n",
    "class RandomJitter(object):\n",
    "    def __init__(self, pixels=2):\n",
    "        self.pixels = pixels\n",
    "        \n",
    "    def __call__(self, img):\n",
    "        dx = np.random.randint(-self.pixels, self.pixels + 1)\n",
    "        dy = np.random.randint(-self.pixels, self.pixels + 1)\n",
    "        return transforms.functional.affine(img, angle=0, translate=(dx, dy), \n",
    "                                         scale=1.0, shear=0)\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        \n",
    "        return self.early_stop\n",
    "    \n",
    "def evaluate_model(model, data_loader, criterion = nn.CrossEntropyLoss()):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return total_loss / len(data_loader), correct / total\n",
    "\n",
    "def save_results(model, history, name):\n",
    "    torch.save(model.state_dict(), f'models/{name}.pt')\n",
    "    pd.DataFrame(history).to_csv(f'logs/{name}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: Model definition\n",
    "class Model(nn.Module):\n",
    "    '''\n",
    "    Used in MNIST experiments.\n",
    "    A two-layer linear ReLU network with dropout and max norm regularization.\n",
    "    '''\n",
    "    def __init__(self, hidden_size, max_norm = 2.0, drop_rate = 0.5):\n",
    "        super(Model, self).__init__()\n",
    "        self.max_norm = max_norm\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28 * 28, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop_rate),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop_rate),\n",
    "            nn.Linear(hidden_size, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Clip the weights to the maximum allowed norm\n",
    "        if self.max_norm is not None:\n",
    "            with torch.no_grad():\n",
    "                for layer in self.modules():\n",
    "                    if isinstance(layer, nn.Linear):\n",
    "                        norm = layer.weight.data.norm(2, dim=1, keepdim=True)\n",
    "                        desired = torch.clamp(norm, max=self.max_norm)\n",
    "                        layer.weight.data *= (desired / norm)\n",
    "                    \n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: Data loading\n",
    "#| echo: false\n",
    "reg_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "aug_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    RandomJitter(pixels = 2),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root = DATA_DIR, train = True,  download = True, transform = aug_transform)\n",
    "test_dataset  = datasets.MNIST(root = DATA_DIR, train = False, download = True, transform = reg_transform)\n",
    "\n",
    "# Split training data into train and validation sets\n",
    "train_size = int(0.9 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    train_dataset, [train_size, val_size]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the distillation loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: Define training losses\n",
    "#| code-fold: show\n",
    "# Regular cross-entropy loss\n",
    "def hard_loss(outputs, labels, criterion, *args):\n",
    "    return criterion(outputs, labels)\n",
    "\n",
    "# Distillation loss\n",
    "def soft_loss(outputs, labels, criterion, examples, big_model, T, a):\n",
    "    with torch.no_grad():\n",
    "        big_model.eval()\n",
    "        soft_labels = F.softmax(big_model(examples) / T, dim = 1)\n",
    "\n",
    "    return a * (T ** 2) * criterion(outputs / T, soft_labels) + (1 - a) * criterion(outputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "def train(model, loss_func, train_loader, val_loader, num_epochs, lr, patience):\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr = lr, momentum = 0.9)\n",
    "    early_stopping = EarlyStopping(patience = patience)\n",
    "    history = collections.defaultdict(list)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = loss_func(outputs, labels, criterion, images)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        # Calculate average training loss\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        val_loss, val_accuracy = evaluate_model(model, val_loader)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_accuracy'].append(val_accuracy)\n",
    "\n",
    "        print(f'epoch: {epoch+1}/{num_epochs}, train_loss: {avg_train_loss:.4f}, val_loss: {val_loss:.4f}, val_accuracy: {val_accuracy:.4f}')\n",
    "        \n",
    "        if early_stopping(val_loss): break\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hidden dimensions of the big and small networks are 1200 and 800 respectively. \n",
    "To train the networks we use an early stopping validation set and choose $T = 4.0$ and $a = 0.5$ (since the authors don't mention their values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: Train the big model\n",
    "#| output: false\n",
    "# Hyperparameters\n",
    "num_epochs = 100\n",
    "batch_size = 128\n",
    "lr = 0.01\n",
    "patience = 7\n",
    "big_model_size = 1200\n",
    "small_model_size = 800\n",
    "\n",
    "loader = lambda ds, shuffle = False: DataLoader(ds, batch_size = batch_size, shuffle = shuffle)\n",
    "val_loader   = loader(val_dataset)\n",
    "test_loader  = loader(test_dataset)\n",
    "\n",
    "# Train the big model\n",
    "train_dataset.dataset.transform = aug_transform\n",
    "train_loader = loader(train_dataset, shuffle = True)\n",
    "\n",
    "big_model = Model(1200).to(device)\n",
    "big_train_history = train(big_model, hard_loss, train_loader, val_loader, num_epochs, lr, patience)\n",
    "test_loss, test_accuracy = evaluate_model(big_model, test_loader)\n",
    "\n",
    "save_results(big_model, big_train_history, 'big_model')\n",
    "test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: Train the smaller model on hard labels\n",
    "#| output: false\n",
    "def train_small_model(train_dataset, val_dataset, seed, loss, model_size = small_model_size):\n",
    "    set_seed(seed)\n",
    "    # no augmentation\n",
    "    train_dataset.dataset.transform = reg_transform\n",
    "    train_loader = loader(train_dataset, shuffle = True)\n",
    "    val_loader = loader(val_dataset)\n",
    "\n",
    "    # or regularization\n",
    "    small_model = Model(model_size, max_norm = None, drop_rate = 0.0).to(device)\n",
    "    small_train_history = train(small_model, loss, train_loader, val_loader, num_epochs, lr, patience)\n",
    "    return small_model, small_train_history\n",
    "\n",
    "# small_model, small_train_history = train_small_model(train_dataset, val_dataset, seed = 42, loss = hard_loss, T = 1.0)\n",
    "# save_results(small_model, small_train_history, 'small_model')\n",
    "# evaluate_model(small_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/100, train_loss: 10.0936, val_loss: 0.1055, val_accuracy: 0.9673\n",
      "epoch: 2/100, train_loss: 9.0607, val_loss: 0.0739, val_accuracy: 0.9755\n",
      "epoch: 3/100, train_loss: 8.9822, val_loss: 0.0632, val_accuracy: 0.9813\n",
      "epoch: 4/100, train_loss: 8.9506, val_loss: 0.0600, val_accuracy: 0.9807\n",
      "epoch: 5/100, train_loss: 8.9331, val_loss: 0.0561, val_accuracy: 0.9827\n",
      "epoch: 6/100, train_loss: 8.9222, val_loss: 0.0570, val_accuracy: 0.9830\n",
      "epoch: 7/100, train_loss: 8.9151, val_loss: 0.0535, val_accuracy: 0.9840\n",
      "epoch: 8/100, train_loss: 8.9089, val_loss: 0.0526, val_accuracy: 0.9840\n",
      "epoch: 9/100, train_loss: 8.9048, val_loss: 0.0519, val_accuracy: 0.9840\n",
      "epoch: 10/100, train_loss: 8.9015, val_loss: 0.0519, val_accuracy: 0.9843\n",
      "epoch: 11/100, train_loss: 8.8989, val_loss: 0.0508, val_accuracy: 0.9848\n",
      "epoch: 12/100, train_loss: 8.8966, val_loss: 0.0505, val_accuracy: 0.9852\n",
      "epoch: 13/100, train_loss: 8.8951, val_loss: 0.0496, val_accuracy: 0.9855\n",
      "epoch: 14/100, train_loss: 8.8933, val_loss: 0.0495, val_accuracy: 0.9853\n",
      "epoch: 15/100, train_loss: 8.8917, val_loss: 0.0487, val_accuracy: 0.9855\n",
      "epoch: 16/100, train_loss: 8.8904, val_loss: 0.0492, val_accuracy: 0.9855\n",
      "epoch: 17/100, train_loss: 8.8894, val_loss: 0.0490, val_accuracy: 0.9850\n",
      "epoch: 18/100, train_loss: 8.8885, val_loss: 0.0486, val_accuracy: 0.9860\n",
      "epoch: 19/100, train_loss: 8.8877, val_loss: 0.0483, val_accuracy: 0.9858\n",
      "epoch: 20/100, train_loss: 8.8870, val_loss: 0.0481, val_accuracy: 0.9860\n",
      "epoch: 21/100, train_loss: 8.8862, val_loss: 0.0480, val_accuracy: 0.9858\n",
      "epoch: 22/100, train_loss: 8.8856, val_loss: 0.0481, val_accuracy: 0.9860\n",
      "epoch: 23/100, train_loss: 8.8849, val_loss: 0.0479, val_accuracy: 0.9858\n",
      "epoch: 24/100, train_loss: 8.8844, val_loss: 0.0474, val_accuracy: 0.9858\n",
      "epoch: 25/100, train_loss: 8.8840, val_loss: 0.0474, val_accuracy: 0.9855\n",
      "epoch: 26/100, train_loss: 8.8838, val_loss: 0.0475, val_accuracy: 0.9857\n",
      "epoch: 27/100, train_loss: 8.8831, val_loss: 0.0475, val_accuracy: 0.9862\n",
      "epoch: 28/100, train_loss: 8.8828, val_loss: 0.0472, val_accuracy: 0.9855\n",
      "epoch: 29/100, train_loss: 8.8825, val_loss: 0.0470, val_accuracy: 0.9873\n",
      "epoch: 30/100, train_loss: 8.8821, val_loss: 0.0470, val_accuracy: 0.9863\n",
      "epoch: 31/100, train_loss: 8.8818, val_loss: 0.0471, val_accuracy: 0.9867\n",
      "epoch: 32/100, train_loss: 8.8815, val_loss: 0.0470, val_accuracy: 0.9867\n",
      "epoch: 33/100, train_loss: 8.8812, val_loss: 0.0470, val_accuracy: 0.9867\n",
      "epoch: 34/100, train_loss: 8.8809, val_loss: 0.0469, val_accuracy: 0.9865\n",
      "epoch: 35/100, train_loss: 8.8807, val_loss: 0.0467, val_accuracy: 0.9870\n",
      "epoch: 36/100, train_loss: 8.8805, val_loss: 0.0466, val_accuracy: 0.9863\n",
      "epoch: 37/100, train_loss: 8.8802, val_loss: 0.0467, val_accuracy: 0.9870\n",
      "epoch: 38/100, train_loss: 8.8800, val_loss: 0.0465, val_accuracy: 0.9872\n",
      "epoch: 39/100, train_loss: 8.8797, val_loss: 0.0465, val_accuracy: 0.9875\n",
      "epoch: 40/100, train_loss: 8.8796, val_loss: 0.0464, val_accuracy: 0.9872\n",
      "epoch: 41/100, train_loss: 8.8794, val_loss: 0.0465, val_accuracy: 0.9872\n",
      "epoch: 42/100, train_loss: 8.8792, val_loss: 0.0467, val_accuracy: 0.9865\n",
      "epoch: 43/100, train_loss: 8.8789, val_loss: 0.0463, val_accuracy: 0.9875\n",
      "epoch: 44/100, train_loss: 8.8787, val_loss: 0.0461, val_accuracy: 0.9870\n",
      "epoch: 45/100, train_loss: 8.8788, val_loss: 0.0462, val_accuracy: 0.9870\n",
      "epoch: 46/100, train_loss: 8.8784, val_loss: 0.0462, val_accuracy: 0.9872\n",
      "epoch: 47/100, train_loss: 8.8784, val_loss: 0.0463, val_accuracy: 0.9877\n",
      "epoch: 48/100, train_loss: 8.8784, val_loss: 0.0463, val_accuracy: 0.9870\n",
      "epoch: 49/100, train_loss: 8.8782, val_loss: 0.0461, val_accuracy: 0.9873\n",
      "epoch: 50/100, train_loss: 8.8780, val_loss: 0.0462, val_accuracy: 0.9873\n",
      "epoch: 51/100, train_loss: 8.8777, val_loss: 0.0460, val_accuracy: 0.9868\n",
      "epoch: 52/100, train_loss: 8.8778, val_loss: 0.0458, val_accuracy: 0.9870\n",
      "epoch: 53/100, train_loss: 8.8777, val_loss: 0.0462, val_accuracy: 0.9873\n",
      "epoch: 54/100, train_loss: 8.8774, val_loss: 0.0460, val_accuracy: 0.9873\n",
      "epoch: 55/100, train_loss: 8.8775, val_loss: 0.0458, val_accuracy: 0.9873\n",
      "epoch: 56/100, train_loss: 8.8774, val_loss: 0.0458, val_accuracy: 0.9867\n",
      "epoch: 57/100, train_loss: 8.8773, val_loss: 0.0458, val_accuracy: 0.9870\n",
      "epoch: 58/100, train_loss: 8.8770, val_loss: 0.0459, val_accuracy: 0.9872\n",
      "epoch: 59/100, train_loss: 8.8771, val_loss: 0.0458, val_accuracy: 0.9868\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.033212841342349204, 0.9891)"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-summary: Train the distilled model\n",
    "#| output: false\n",
    "temperature, a = 4.0, 0.5\n",
    "\n",
    "distilled_model, distilled_train_history = train_small_model(\n",
    "    train_dataset = train_dataset, val_dataset = val_dataset, seed = 42,\n",
    "    loss = functools.partial(soft_loss, big_model = big_model, T = temperature, a = a)\n",
    ")\n",
    "save_results(distilled_model, distilled_train_history, 'distilled_model')\n",
    "evaluate_model(distilled_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0r/dny0wtpd4412y54gp84twsj40000gn/T/ipykernel_76611/4017433887.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('models/big_model.pt'))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: false\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize = (8, 4))\n",
    "\n",
    "# plot an example in the fist ax\n",
    "idx = 9694\n",
    "img, label = test_dataset[idx]\n",
    "ax1.imshow(img.squeeze().numpy(), cmap = 'gray')\n",
    "ax1.axis('off')\n",
    "ax1.set_title(f'Input')\n",
    "\n",
    "# ax2: the predicted distribution and hard label\n",
    "# Load the big model and get the output\n",
    "model = Model(big_model_size).to(device)\n",
    "model.load_state_dict(torch.load('models/big_model.pt'))\n",
    "model.eval()\n",
    "get_probs = lambda o, T: F.softmax(o / T, dim = 1)[0].cpu().numpy()\n",
    "with torch.no_grad():\n",
    "    output = model(img.to(device)[None, :])\n",
    "\n",
    "# Prepare data for seaborn\n",
    "data = pd.DataFrame({\n",
    "    'Class': list(range(10)) * 3,\n",
    "    'Probability': [1.0 if i == label else 0.0 for i in range(10)] + list(get_probs(output, 4.0)) + list(get_probs(output, 8.0)),\n",
    "    'Type': ['Hard'] * 10 + ['Soft (T=4.0)'] * 10 + ['Soft (T=8.0)'] * 10\n",
    "})\n",
    "\n",
    "# Plot with seaborn\n",
    "sns.barplot(x='Class', y='Probability', hue='Type', data=data, ax=ax2)\n",
    "ax2.set_ylabel(''); ax2.set_xlabel('')\n",
    "# ax2.set_xticks(range(10))\n",
    "ax2.set_title(f'Labels')\n",
    "\n",
    "f.tight_layout()\n",
    "f.savefig('images/demo.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And get the following test accuracies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0r/dny0wtpd4412y54gp84twsj40000gn/T/ipykernel_76611/2388879164.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f'models/{model_name}.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "big: 0.9901, small: 0.9833, distilled: 0.9891\n"
     ]
    }
   ],
   "source": [
    "#| echo: false\n",
    "names = ['big_model', 'small_model', 'distilled_model']\n",
    "res = []\n",
    "for model_name in names:\n",
    "    model = Model(big_model_size if 'big' in model_name else small_model_size).to(device)\n",
    "    model.load_state_dict(torch.load(f'models/{model_name}.pt'))\n",
    "    model.eval()\n",
    "    _, test_accuracy = evaluate_model(model, test_loader)\n",
    "    res.append(f'{model_name.split(\"_\")[0]}: {test_accuracy:.4f}')\n",
    "\n",
    "print(', '.join(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mystical 3\n",
    "\n",
    "The authors then remove 3 from the transfer set the distilled model is trained on to test its generalization to unseen classes. *\"So from the perspective\n",
    "of the distilled model, 3 is a mythical digit that it has never seen*\". When we evaluate on the test set, which still contains 3s, we see that the distilled model performs much better than a small model trained with hard labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: Train without 3s in transfer set\n",
    "#| output: false\n",
    "\n",
    "# Remove all 3s from the dataset\n",
    "train_dataset = datasets.MNIST(root = DATA_DIR, train = True,  download = True, transform = aug_transform)\n",
    "train_dataset = torch.utils.data.Subset(train_dataset, np.where(train_dataset.targets != 3)[0])\n",
    "\n",
    "# Split training data into train and validation sets\n",
    "train_size = int(0.9 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    train_dataset, [train_size, val_size]\n",
    ")\n",
    "\n",
    "# Train small without distillation\n",
    "small_no_3, small_no_3_history = train_small_model(\n",
    "    train_dataset = train_dataset, val_dataset = val_dataset,\n",
    "    seed = 42, loss = hard_loss, model_size = 800\n",
    ")\n",
    "\n",
    "# Train small with distillation\n",
    "distilled_no_3, distilled_no_3_history = train_small_model(\n",
    "    train_dataset = train_dataset, val_dataset = val_dataset,\n",
    "    seed = 42, loss = functools.partial(soft_loss, big_model = big_model, T = 4.0, a = 0.5),\n",
    "    model_size = 800\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0r/dny0wtpd4412y54gp84twsj40000gn/T/ipykernel_76611/1087436920.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('models/small_no_3.pt'))\n",
      "/var/folders/0r/dny0wtpd4412y54gp84twsj40000gn/T/ipykernel_76611/1087436920.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('models/distilled_no_3.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not distilled: 0.8882, distilled: 0.9869\n"
     ]
    }
   ],
   "source": [
    "#| code-summary: Eval models\n",
    "#| echo: false\n",
    "model = Model(800).to(device)\n",
    "model.load_state_dict(torch.load('models/small_no_3.pt'))\n",
    "model.eval()\n",
    "_, not_dist_acc = evaluate_model(model, test_loader)\n",
    "\n",
    "model = Model(800).to(device)\n",
    "model.load_state_dict(torch.load('models/distilled_no_3.pt'))\n",
    "model.eval()\n",
    "_, dist_acc = evaluate_model(model, test_loader)\n",
    "\n",
    "print(f'small: {not_dist_acc:.4f}, distilled: {dist_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the paper, the authors take it to the extreme and show that a distilled model trained only on 7 and 8 still achieves impressive performance. They also do experiments on a bigger speech recognition dataset and discuss training experts on a CV dataset with distillation from a generalist model as regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final thoughts\n",
    "\n",
    "It was very fun to return to this classic paper. It introduced a simple yet powerful idea that is still widely used today. Like most of these papers (circa 2015), it is very clear and readable. Andas Hinton stapleit is slightly bio-inspired, in this case by larvae.\n",
    "\n",
    "Some pointers to papers that extended on this idea. [Self-distillation](https://arxiv.org/abs/1805.04770) makes the teacher (\"big\") and student (\"small\") models the same size, and in [mutual learning](https://arxiv.org/abs/1706.00384) two or more networks learn collaboratively. However, the main extensions of this paper build on its main theme: train on a richer signal. You might train the student to imitate the teacher's [intermediate](https://arxiv.org/abs/1412.6550) (or [last](https://openreview.net/forum?id=ZzwDy_wiWv)) representations, [attention maps](https://arxiv.org/abs/1612.03928), etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
