{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"SALSA: Soup-based Alignment Learning for Stronger Adaptation in RLHF\"\n",
    "subtitle: \"Intuitions and partial reproduction\"\n",
    "date: 2025-01-15\n",
    "categories: [deep learning, paper]\n",
    "format:\n",
    "  html:\n",
    "    other-links:\n",
    "      - text: Paper\n",
    "        icon: book\n",
    "        href: https://arxiv.org/abs/2411.01798\n",
    "    toc: true\n",
    "draft: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea\n",
    "\n",
    "## Reward Analysis\n",
    "\n",
    "## Ablation\n",
    "\n",
    "## Do we have to soup?\n",
    "\n",
    "## KL-Hack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-summary: Imports and model evaluation function\n",
    "#| output: false\n",
    "#| echo: false\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os, functools, itertools\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "for d in ['data', 'models', 'logs']: os.makedirs(d, exist_ok = True)\n",
    "\n",
    "device = torch.device(\n",
    "    'cuda' if torch.cuda.is_available() else\n",
    "    ('mps' if torch.backends.mps.is_available() else\n",
    "    'cpu')\n",
    ")\n",
    "\n",
    "def set_seed(seed = 42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if device.type == 'cuda':\n",
    "        print('Setting seed for CUDA')\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|code-summary: Get model and SFT data\n",
    "\n",
    "def format_chat(example):\n",
    "    \"\"\"Format UltraChat example into chat format\"\"\"\n",
    "    conversation = f\"<start_of_turn>user\\n{example['prompt']}\\n<end_of_turn>\\n\"\n",
    "    \n",
    "    for msg in example['messages']:\n",
    "        conversation += f\"<start_of_turn>{msg['role']}\\n{msg['content']}\\n<end_of_turn>\\n\"\n",
    "    \n",
    "    return {\"text\": conversation.strip()}\n",
    "\n",
    "model_name = 'HuggingFaceTB/SmolLM2-135M'\n",
    "get_model = lambda: AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "model = get_model()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "sft_data = load_dataset('HuggingFaceH4/ultrachat_200k')\n",
    "format_dataset = lambda dataset: dataset.map(format_chat, remove_columns = dataset.column_names)\n",
    "sft_train, sft_test = format_dataset(sft_data['train_sft']), format_dataset(sft_data['test_sft'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5012bdf6ffdf4c2e8465636c912b94e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57642 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sft_config = SFTConfig(\n",
    "    dataset_text_field = 'text',\n",
    "    learning_rate = 2e-5,\n",
    "    per_device_eval_batch_size = 1,\n",
    "    gradient_accumulation_steps = 16,\n",
    "    max_seq_length = 2048,\n",
    "    output_dir = 'models/sft',\n",
    "    bf16 = True,\n",
    "    packing = True,\n",
    "    gradient_checkpointing=True\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset = sft_train,\n",
    "    eval_dataset = sft_test,\n",
    "    args = sft_config,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
