{
 "cells": [
  {
   "cell_type": "raw",
   "id": "282034e4",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: 'Grokking the Gumbel-Softmax trick'\n",
    "subtitle: 'or how to backprop through sampling a discrete distribution'\n",
    "date: today\n",
    "date-modified: today\n",
    "categories: [quick]\n",
    "format:\n",
    "    html:\n",
    "        code-fold: false\n",
    "draft: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bfea5b",
   "metadata": {},
   "source": [
    "## Discrete sampling as noise + argmax\n",
    "\n",
    "Say you have log-probabilities (logits) $\\phi_i$ for a categorical distribution $\\pi$ you want to use to generate a sample $z$. You could undo the log and normalize the logits to sample $z$ directly from the density\n",
    "\n",
    "$$\n",
    "\\pi_i = \\frac{\\exp(\\phi_i)}{\\sum_j \\exp(\\phi_j)}\n",
    "$$\n",
    "\n",
    "However, it turns out that you can achieve the same by adding i.i.d noise $g_i \\sim \\text{Gumbel}(0, 1)$ to the logits and perform an argmax\n",
    "\n",
    "\n",
    "$$\n",
    "P \\left[ \\text{arg max}_j  \\phi_j + g_j = i \\right] = \\pi_i\n",
    "$$\n",
    "\n",
    "this is called the Gumbel-Max trick. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "073b9a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "#| output: false\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "85b63f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1502, 0.3990, 0.4507])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: false\n",
    "# Categorical distribution\n",
    "pi = torch.tensor([0.15, 0.4, 0.45])\n",
    "logits = torch.log(pi)\n",
    "\n",
    "N = 100_000     # no. of samples\n",
    "C = pi.shape[0] # no. of categories\n",
    "\n",
    "# Now the trick\n",
    "def gumbel_noise(shape): # Gumbel(0,1)\n",
    "    u = torch.rand(shape)\n",
    "    return -torch.log(-torch.log(u))\n",
    "\n",
    "\n",
    "g = gumbel_noise((N, C))\n",
    "sample = torch.argmax(logits + g, dim = -1)\n",
    "\n",
    "# Verify\n",
    "sample.unique(return_counts=True)[1] / N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a29a891",
   "metadata": {},
   "source": [
    "## Differentiable soft sampling\n",
    "\n",
    "By itself, the Gumbel-Max trick is not very useful (verify?). However if we replace the argmax by its differentiable approximation — the [soft(arg)max](https://en.wikipedia.org/wiki/Softmax_function#Smooth_arg_max) — we gain the ability to backpropagate through the sampling.\n",
    "\n",
    "$$\n",
    "\\pi_i \\approx \\text{softmax}_\\tau(\\phi_i + g_i) = \\frac{\\exp((\\phi_i + g_i)/\\tau)}{\\sum_j \\exp((\\phi_j + g_i) / \\tau)}\n",
    "$$\n",
    "\n",
    "\n",
    "We also introduce a \"temperature\" parameter $\\tau$ that controls how accurately we .... With $\\tau \\to 0$ the output becomes the one-hot encoded class and when $\\tau \\to \\infty$ the output approaches the uniform.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f63472e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0200, 0.9800],\n",
       "        [0.0000, 0.4300, 0.5700],\n",
       "        [1.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.0700, 0.0000, 0.9300],\n",
       "        [0.0000, 1.0000, 0.0000],\n",
       "        [0.0000, 0.9200, 0.0800]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = 0.1\n",
    "g = gumbel_noise((N, C))\n",
    "soft_sample = F.softmax((logits + g) / temp, dim = -1)\n",
    "\n",
    "torch.round(soft_sample, decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf0d723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1511, 0.4001, 0.4489])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_sample.mean(dim=0) # close to pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf90831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 \t tensor([0.1496, 0.3990, 0.4513])\n",
      "0.05 \t tensor([0.1497, 0.4006, 0.4498])\n",
      "0.1 \t tensor([0.1508, 0.4000, 0.4492])\n",
      "1 \t tensor([0.1951, 0.3871, 0.4178])\n",
      "10 \t tensor([0.3107, 0.3428, 0.3465])\n"
     ]
    }
   ],
   "source": [
    "for temp in [0.01, 0.1, 1, 10]:\n",
    "    g = gumbel_noise((N, C))\n",
    "    soft_sample = F.softmax((logits + g) / temp, dim = -1)\n",
    "    print(temp, '\\t', soft_sample.mean(dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795246ef",
   "metadata": {},
   "source": [
    "- why keep the gumbel noise?\n",
    "- applications??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169b87d5",
   "metadata": {},
   "source": [
    "As you can see, because of the softmax, we get soft or fuzzy sampling. That is, we don't get a single class but a distribution over them as output.\n",
    "\n",
    "TODO some applications here\n",
    "\n",
    "\n",
    "\n",
    "## Discrete and differentiable sampling\n",
    "\n",
    "However, some applications require we discretize and actually just choose one element from the distribution *and* we be able to backpropagate through the sampling. We could try taking the argmax of the softmax output, but we are back where we started since the argmax is not differentiable. \n",
    "\n",
    "\n",
    "The trick is to use the straight-through estimator, that is ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f2a7f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1509, 0.3994, 0.4497])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gumb_softmax_sample = F.softmax((logits + g) / temp, dim = -1)\n",
    "gumb_hard_sample = torch.argmax(gumb_softmax_sample, dim=-1)\n",
    "\n",
    "# Straight-through estimator\n",
    "\n",
    "forward = hard_value\n",
    "gradients = soft_value\n",
    "x = (hard_value - soft_value).detach() + soft_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b6c787",
   "metadata": {},
   "source": [
    "## Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e47ef5",
   "metadata": {},
   "source": [
    "Sources: \n",
    "- https://arxiv.org/pdf/1611.01144\n",
    "- https://github.com/pytorch/pytorch/blob/v2.7.0/torch/nn/functional.py#L2146"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
