<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-05-06">

<title>Engression – Emilio Cantu-Cervini</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-6bd9cfa162949bde0a231f530c97869d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-H4ZD13L2K2"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-H4ZD13L2K2', { 'anonymize_ip': true});
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../index.html"> 
<span class="menu-text">home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../posts/index.html"> 
<span class="menu-text">posts</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#in-a-nutshell" id="toc-in-a-nutshell" class="nav-link active" data-scroll-target="#in-a-nutshell">In a nutshell</a></li>
  <li><a href="#pre-additive-noise-and-extrapolation" id="toc-pre-additive-noise-and-extrapolation" class="nav-link" data-scroll-target="#pre-additive-noise-and-extrapolation">Pre-additive noise and extrapolation</a></li>
  <li><a href="#implementation" id="toc-implementation" class="nav-link" data-scroll-target="#implementation">Implementation</a></li>
  <li><a href="#hyperparameters" id="toc-hyperparameters" class="nav-link" data-scroll-target="#hyperparameters">Hyperparameters</a></li>
  <li><a href="#real-data" id="toc-real-data" class="nav-link" data-scroll-target="#real-data">Real data</a></li>
  <li><a href="#final-thoughts" id="toc-final-thoughts" class="nav-link" data-scroll-target="#final-thoughts">Final thoughts</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/emiliocantuc/emiliocantuc.github.io/blob/main/posts/papers/engression/index.ipynb" class="toc-action"><i class="bi bi-github"></i>View source</a></li></ul></div><div class="quarto-other-links"><h2>Other Links</h2><ul><li><a href="https://arxiv.org/abs/2307.00835"><i class="bi bi-book"></i>Paper</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Engression</h1>
<p class="subtitle lead">exploring a lightweight approach to distributional regression</p>
  <div class="quarto-categories">
    <div class="quarto-category">paper</div>
  </div>
  </div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">May 6, 2025</p>
    </div>
  </div>
  
    <div>
    <div class="quarto-title-meta-heading">Modified</div>
    <div class="quarto-title-meta-contents">
      <p class="date-modified">September 17, 2025</p>
    </div>
  </div>
    
  </div>
  


</header>


<p>Traditional regression models predict the conditional mean <span class="math inline">\(\mathbb E[Y∣X=x]\)</span>, or <a href="https://en.wikipedia.org/wiki/Quantile_regression">sometimes</a> a few quantiles. In contrast, distributional regression attempt to learn the <em>entire</em> conditional distribution <span class="math inline">\(Y|X=x\)</span>. Having access to the full distribution gives us calibrated uncertainty estimates, probabilistic forecasts, etc.</p>
<p>In a recent seminar I learned about engression, a lightweight and principled approach to distributional regression. Instead of predicting a parametric distribution or optimizing a likelihood, engression trains models to transform noise into samples from <span class="math inline">\(Y|X=x\)</span>, using a <a href="https://en.wikipedia.org/wiki/Scoring_rule">proper scoring rule</a> called the <a href="https://sites.stat.washington.edu/raftery/Research/PDF/Gneiting2007jasa.pdf">energy score</a>. It’s implicit, generative, and remarkably straightforward to implement.</p>
<p>Here, I try to explain the main idea, reproduce some of the <a href="https://arxiv.org/abs/2307.00835">paper</a>’s results, and discuss a few of its properties.</p>
<section id="in-a-nutshell" class="level2">
<h2 class="anchored" data-anchor-id="in-a-nutshell">In a nutshell</h2>
<p>We consider a general class of models <span class="math inline">\(\mathcal{M} = {g(x, \varepsilon)}\)</span>, where each model takes covariates <span class="math inline">\(x\)</span> and a random vector <span class="math inline">\(\varepsilon\)</span> as input. The noise vector <span class="math inline">\(\varepsilon\)</span> is drawn from a fixed distribution independent of <span class="math inline">\(x\)</span>. We imagine each function <span class="math inline">\(g(x, \varepsilon) \mapsto y\)</span> in <span class="math inline">\(\mathcal{M}\)</span> as defining a conditional distribution of <span class="math inline">\(Y \mid X = x\)</span>. The “best” <span class="math inline">\(g\)</span> is found by minimizing the <strong>engression loss</strong>: <span class="math display">\[
\mathbb E \left[ \left \| Y - g(X, \varepsilon) \right \| - \frac{1}{2} \left \| g(X, \varepsilon) - g(X, \varepsilon') \right \| \right]
\]</span></p>
<p>where <span class="math inline">\(\varepsilon\)</span> and <span class="math inline">\(\varepsilon'\)</span> are independent draws from the noise distribution. This is the negative of the energy score, a proper scoring rule. As a result, the authors show that the minimizer recovers the true conditional distribution <span class="math inline">\(Y \mid X\)</span> — assuming the model class <span class="math inline">\(\mathcal{M}\)</span> is expressive enough. In practice, this means using neural networks with sufficient capacity.</p>
<p>Intuitively, the loss encourages two things. The first term ensures that the generated samples are close to the observed target. It pulls the predicted distribution toward the actual data. The second term penalizes collapsing all samples to a single point. It forces the model to generate diverse samples, reflecting the variability in the data.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/loss.gif" class="img-fluid figure-img"></p>
<figcaption>If we only used the first term in the engression loss, our estimated distribution would collapse. Toy example where <span class="math inline">\(Y|X \sim N(X, 0.1)\)</span></figcaption>
</figure>
</div>
<p>To minimize the empirical version of the loss using a dataset <span class="math inline">\(\{(X_i, Y_i): i=1, \dots, n\}\)</span>, we sample <span class="math inline">\(m\)</span> noise vectors <span class="math inline">\(\varepsilon\)</span> <em>per</em> observation and minimize</p>
<p><span class="math display">\[
\frac{1}{n} \left [ \frac{1}{m} \sum_{j=1}^m  \| Y_i - g(X_i, \varepsilon_{i,j}) \| - \frac{1}{2m(m-1)} \sum_{j=1}^m \sum_{j'=1}^m \|g(X_i, \varepsilon_{i, j}) - g(X_i, \varepsilon_{i, j'})  \|\right]
\]</span></p>
<p>Once trained, <span class="math inline">\(g\)</span> acts as an <em>implicit</em> and <em>generative</em> model for <span class="math inline">\(Y \mid X\)</span>. It doesn’t yield an explicit density <span class="math inline">\(p(y \mid x)\)</span>, but we can generate samples by sampling <span class="math inline">\(\varepsilon\)</span> and feeding it into <span class="math inline">\(g\)</span>. From these samples, we can estimate means, medians, confidence intervals, etc. as usual.</p>
<p>Note: While we focused on the main loss explored in the paper, the authors mention several generalizations in Appendix D. Specifically, the energy score is one example of a broader class of kernel scores, and engression can in principle use any proper scoring rule that characterizes a distribution (see Section 2). I wouldn’t be surprised if future work develops variants that emphasize the tails, which could be especially useful in risk-sensitive applications.</p>
</section>
<section id="pre-additive-noise-and-extrapolation" class="level2">
<h2 class="anchored" data-anchor-id="pre-additive-noise-and-extrapolation">Pre-additive noise and extrapolation</h2>
<p>While minimizing the loss above is sufficient to learn the conditional distribution within the training range (assuming <span class="math inline">\(g\)</span> is expressive enough), the authors show that, under certain assumptions about the noise structure, engression can also support limited extrapolation.</p>
<p>Most regression and generative models assume that the noise is post-additive. That is, that noise <span class="math inline">\(\eta\)</span> is added after applying a nonlinear transformation to the covariates: <span class="math inline">\(Y = g(X) + \eta\)</span>. Pre-additive noise instead assumes <span class="math inline">\(Y = g(X + \eta)\)</span>. This helps with extrapolation because, as the authors note</p>
<blockquote class="blockquote">
<p>“As such, if the data are generated according to a post-ANM, the observations for the response variable are perturbed values of the true function evaluated at covariate values within the support. We hence generally have no data-driven information about the behaviour of the true function outside the support. In contrast, data generated from a pre-ANM contain response values that reveal some information beyond the support”</p>
</blockquote>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="images/noise.png" class="img-fluid quarto-figure quarto-figure-left figure-img"></p>
</figure>
</div>
<p>The authors formalize this and show that under certain structural assumptions — like smoothness or monotonicity of <span class="math inline">\(g\)</span>, and symmetric pre-additive noise — engression can recover aspects of <span class="math inline">\(g\)</span> beyond the training range. A key idea is that larger input noise gives you more indirect coverage of nearby regions, so the model can “see” a bit past the edge of the data. I won’t go into the technical details here, but the extrapolation results are interesting and worth checking out if you’re curious.</p>
</section>
<section id="implementation" class="level2">
<h2 class="anchored" data-anchor-id="implementation">Implementation</h2>
<p>We walk through how simple engression is to implement and attempt to reproduce Figure 4 from the paper, which highlights its extrapolation capabilities using synthetic data.</p>
<p>We implement <span class="math inline">\(g(x, \varepsilon) \mapsto y\)</span> as a MLP. In the paper, the authors feed the concatenated vector <span class="math inline">\([x, \varepsilon]\)</span> into a standard (deterministic) network. To keep things flexible and modular — and since <span class="math inline">\(\varepsilon\)</span> must be sampled independently of <span class="math inline">\(x\)</span> — we can instead:</p>
<div id="485baf7e" class="cell" data-execution_count="3">
<details open="" class="code-fold">
<summary>Defining g(x, eps)</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> gConcatenate(nn.Module):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">""" g(x, eps) = g([x, eps]), where eps ~ N(0, 1) or Unif(0, 1) """</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, model, noise_dim, noise_type <span class="op">=</span> <span class="st">'normal'</span>, scale <span class="op">=</span> <span class="fl">1.0</span>):</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(gConcatenate, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> model</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.noise_dim <span class="op">=</span> noise_dim</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.noise_f <span class="op">=</span> torch.randn <span class="cf">if</span> noise_type <span class="op">==</span> <span class="st">'normal'</span> <span class="cf">else</span> torch.rand</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scale <span class="op">=</span> scale</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> <span class="bu">len</span>(x.shape) <span class="op">&gt;=</span> <span class="dv">2</span>,<span class="st">'x must have at least 2 dims, where batch is the first'</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        eps <span class="op">=</span> <span class="va">self</span>.noise_f(x.shape[<span class="dv">0</span>], <span class="va">self</span>.noise_dim).to(x.device)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>        eps <span class="op">=</span> <span class="va">self</span>.scale <span class="op">*</span> eps</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.model(torch.cat([x, eps], dim <span class="op">=</span> <span class="dv">1</span>))</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co"># example</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> MLP(<span class="dv">1</span>, [<span class="dv">64</span>, <span class="dv">64</span>], <span class="dv">1</span>)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> gConcatenate(model, noise_dim <span class="op">=</span> <span class="dv">100</span>, noise_type <span class="op">=</span> <span class="st">'uniform'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Of course, there are many different ways one could define <span class="math inline">\(g\)</span> beyond concatenating the noise with the input. I’m sure future work will explore them.</p>
<p>Now, define the loss function:</p>
<div id="24ad86f1" class="cell" data-execution_count="4">
<details open="" class="code-fold">
<summary>engression loss</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> engression_loss(y, preds, p <span class="op">=</span> <span class="dv">2</span>, return_terms <span class="op">=</span> <span class="va">False</span>):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co">        y (torch.Tensor): True target values (batch_size, output_dim).</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co">        preds (torch.Tensor): Predicted target with independently sampled noise (batch_size, m_samples, output_dim).</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co">        p (float): The order of the norm to use for the loss calculation.</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co">        return_terms (bool): If True, return the individual terms of the loss.</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> preds.shape[<span class="dv">0</span>] <span class="op">==</span> y.shape[<span class="dv">0</span>] <span class="kw">and</span> preds.shape[<span class="dv">2</span>] <span class="op">==</span> y.shape[<span class="dv">1</span>]</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    n, m, D <span class="op">=</span> preds.shape <span class="co"># n is batch_size, m is num_samples, D is output_dim</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Term 1: the absolute error between the predicted and true values</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    term1 <span class="op">=</span> torch.linalg.vector_norm(preds <span class="op">-</span> y[:, <span class="va">None</span>, :], <span class="bu">ord</span> <span class="op">=</span> p, dim <span class="op">=</span> <span class="dv">2</span>).mean()</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Term 2: pairwise absolute differences between the predicted values</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    term2 <span class="op">=</span> torch.tensor(<span class="fl">0.0</span>, device <span class="op">=</span> preds.device, dtype <span class="op">=</span> preds.dtype)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> m <span class="op">&gt;</span> <span class="dv">1</span>:</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># cdist is convinient. The result shape before sum is (n, m, m).</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>        sum_pairwise_l1_dists <span class="op">=</span> torch.cdist(preds, preds, p <span class="op">=</span> p).<span class="bu">sum</span>()</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>        term2 <span class="op">=</span> <span class="op">-</span> sum_pairwise_l1_dists <span class="op">/</span> (n <span class="op">*</span> <span class="dv">2</span> <span class="op">*</span> m <span class="op">*</span> (m <span class="op">-</span> <span class="dv">1</span>) <span class="op">*</span> D)</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (term1 <span class="op">+</span> term2, term1, term2) <span class="cf">if</span> return_terms <span class="cf">else</span> term1 <span class="op">+</span> term2</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>And a simple training loop:</p>
<div id="f6ca290b" class="cell" data-execution_count="5">
<details open="" class="code-fold">
<summary>Training loop</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(g, dl, m, lr <span class="op">=</span> <span class="fl">0.001</span>, epochs <span class="op">=</span> <span class="dv">100</span>, verbose <span class="op">=</span> <span class="va">True</span>):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> torch.optim.Adam(g.parameters(), lr <span class="op">=</span> lr)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> []</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> x, y <span class="kw">in</span> dl:</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>            g.zero_grad()</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Generate m samples from the model</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>            <span class="co"># shape: (batch_size, m, output_dim)</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>            preds <span class="op">=</span> torch.stack([g(x) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(m)], dim <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> engression_loss(y, preds)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>            optimizer.step()</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>            losses.append(loss.item())</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> verbose: <span class="bu">print</span>(loss.item())</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> losses</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Finally, we attempt to replicate Figure 4 from the paper using simulated data with pre-additive noise. For the <code>softplus</code> case, for example, we generate <span class="math inline">\(X \sim \text{Unif}[-2, 2]\)</span>, <span class="math inline">\(\eta \sim \mathcal N(0, 1)\)</span> and set <span class="math inline">\(Y = g^\ast(x + \eta)\)</span> with <span class="math inline">\(g^\ast(t) = \log(1+\exp(t))\)</span>. We then train simple MLPs using the engression loss and evaluate their ability to recover the true median — given by <span class="math inline">\(g^\ast(x)\)</span> — especially outside the range of the training data.</p>
<div id="63f80c15" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Synthetic data</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>g_stars <span class="op">=</span> {</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">'softplus'</span>: F.softplus,</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">'square'</span>: <span class="kw">lambda</span> x: (torch.<span class="bu">max</span>(x, torch.tensor(<span class="fl">0.0</span>)) <span class="op">**</span> <span class="dv">2</span>) <span class="op">/</span> <span class="dv">2</span>,</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">'cubic'</span>: <span class="kw">lambda</span> x: (x <span class="op">**</span> <span class="dv">3</span>) <span class="op">/</span> <span class="dv">3</span>,</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">'log'</span>: <span class="kw">lambda</span> x: (x<span class="op">/</span><span class="dv">3</span> <span class="op">+</span> math.log(<span class="dv">3</span>) <span class="op">-</span> <span class="dv">2</span><span class="op">/</span><span class="dv">3</span>)<span class="op">*</span>(x <span class="op">&lt;=</span> <span class="dv">2</span>) <span class="op">+</span> (torch.log(<span class="dv">1</span> <span class="op">+</span> x<span class="op">*</span>(x <span class="op">&gt;</span> <span class="dv">2</span>)))<span class="op">*</span>(x <span class="op">&gt;</span> <span class="dv">2</span>) </span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>x_train_lims <span class="op">=</span> {</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">'softplus'</span>: (<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>),</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'square'</span>: (<span class="dv">0</span>, <span class="dv">2</span>),</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">'cubic'</span>: (<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>),</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">'log'</span>: (<span class="dv">0</span>, <span class="dv">2</span>)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>x_test_lims <span class="op">=</span> {</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">'softplus'</span>: (<span class="op">-</span><span class="dv">2</span>, <span class="dv">8</span>),</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">'square'</span>: (<span class="dv">0</span>, <span class="dv">6</span>),</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>    <span class="st">'cubic'</span>: (<span class="op">-</span><span class="dv">2</span>, <span class="dv">6</span>),</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    <span class="st">'log'</span>: (<span class="dv">0</span>, <span class="dv">10</span>)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_data(n, input_dim, <span class="bu">type</span> <span class="op">=</span> <span class="st">'softplus'</span>, pre_additive_noise <span class="op">=</span> <span class="va">True</span>):</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>    g_star <span class="op">=</span> g_stars[<span class="bu">type</span>]</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>    a, b <span class="op">=</span> x_train_lims[<span class="bu">type</span>]</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> torch.rand((n, input_dim)) <span class="op">*</span> (b <span class="op">-</span> a) <span class="op">+</span> a <span class="co"># Unif(a, b)</span></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>    sd <span class="op">=</span> <span class="dv">1</span> <span class="cf">if</span> <span class="bu">type</span> <span class="op">!=</span> <span class="st">'cubic'</span> <span class="cf">else</span> <span class="fl">1.1</span></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>    eta <span class="op">=</span> torch.randn((n, input_dim)) <span class="op">*</span> sd</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> pre_additive_noise:</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>        Y <span class="op">=</span> g_star(X <span class="op">+</span> eta)</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>        Y <span class="op">=</span> g_star(X) <span class="op">+</span> eta</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> X, Y</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a><span class="co"># example</span></span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>X, Y <span class="op">=</span> train_data(<span class="dv">1000</span>, <span class="dv">1</span>, <span class="bu">type</span> <span class="op">=</span> <span class="st">'softplus'</span>, pre_additive_noise <span class="op">=</span> <span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="e01519ce" class="cell" data-execution_count="60">
<details class="code-fold">
<summary>Figure 4</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>input_dim, hidden_dim, output_dim <span class="op">=</span> <span class="dv">1</span>, <span class="dv">128</span>, <span class="dv">1</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>noise_dim, m_train, m_pred <span class="op">=</span> <span class="dv">100</span>, <span class="dv">2</span>, <span class="dv">512</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">1024</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>ds_size <span class="op">=</span> <span class="kw">lambda</span> name: <span class="dv">50_000</span> <span class="cf">if</span> name <span class="kw">in</span> <span class="st">'softplus/square'</span> <span class="cf">else</span> <span class="dv">100_000</span> <span class="co"># 50k, 100k</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>depth <span class="op">=</span> <span class="kw">lambda</span> name: <span class="dv">2</span> <span class="cf">if</span> name <span class="kw">in</span> <span class="st">'softplus/square'</span> <span class="cf">else</span> <span class="dv">3</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>hidden_dim <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="kw">lambda</span> name: <span class="fl">5e-2</span> <span class="co">#1e-3 #if name in 'softplus/square' else 1e-4</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>n_runs <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>pre_additive_noise <span class="op">=</span> <span class="va">True</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>saved_g_preds <span class="op">=</span> {}</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>saved_l1_preds <span class="op">=</span> {}</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>extra <span class="op">=</span> <span class="st">'_post'</span> <span class="cf">if</span> <span class="kw">not</span> pre_additive_noise <span class="cf">else</span> <span class="st">''</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>saved_g_preds <span class="op">=</span> torch.load(<span class="ss">f'logs/saved_g_preds</span><span class="sc">{</span>extra<span class="sc">}</span><span class="ss">.pt'</span>)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>saved_l1_preds <span class="op">=</span> torch.load(<span class="ss">f'logs/saved_l1_preds</span><span class="sc">{</span>extra<span class="sc">}</span><span class="ss">.pt'</span>)</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>f, axs <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, figsize <span class="op">=</span> (<span class="dv">7</span>, <span class="dv">7</span>))</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (name, g_star), ax <span class="kw">in</span> <span class="bu">zip</span>(g_stars.items(), axs.flatten()):</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print(name)</span></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>    t <span class="op">=</span> torch.linspace(x_test_lims[name][<span class="dv">0</span>], x_test_lims[name][<span class="dv">1</span>], <span class="dv">25</span>)[:, <span class="va">None</span>]</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Cache to fix plot, etc</span></span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> name <span class="kw">in</span> saved_g_preds:</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>        g_preds <span class="op">=</span> saved_g_preds[name]</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>        g_preds <span class="op">=</span> []</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> seed <span class="kw">in</span> <span class="bu">range</span>(n_runs):</span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>            set_seed(seed)</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>            <span class="co"># model = MLP(input_dim + noise_dim, [hidden_dim] * depth(name), output_dim)</span></span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>            model <span class="op">=</span> ResMLP(input_dim <span class="op">+</span> noise_dim, hidden_dim, depth(name), output_dim)</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>            g <span class="op">=</span> gConcatenate(model, noise_dim <span class="op">=</span> noise_dim, noise_type <span class="op">=</span> <span class="st">'normal'</span>)</span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a>            X, Y <span class="op">=</span> train_data(ds_size(name), input_dim, <span class="bu">type</span> <span class="op">=</span> name, pre_additive_noise <span class="op">=</span> pre_additive_noise)</span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a>            dl <span class="op">=</span> torch.utils.data.DataLoader(<span class="bu">list</span>(<span class="bu">zip</span>(X, Y)), batch_size <span class="op">=</span> batch_size <span class="cf">if</span> batch_size <span class="cf">else</span> <span class="bu">len</span>(Y), shuffle <span class="op">=</span> <span class="va">True</span>, drop_last <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a>            losses <span class="op">=</span> train(g, dl, m_train, epochs <span class="op">=</span> epochs, verbose <span class="op">=</span> <span class="va">False</span>, lr <span class="op">=</span> lr(name))</span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Predict the median using g: sample m_pred per point and take the median</span></span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a>            <span class="cf">with</span> torch.no_grad():</span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a>                g.<span class="bu">eval</span>()</span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a>                g_pred <span class="op">=</span> torch.quantile(</span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a>                    torch.stack([g(t) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(m_pred)], dim <span class="op">=</span> <span class="dv">1</span>), q <span class="op">=</span> <span class="fl">0.5</span>, dim <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a>                g_preds.append(g_pred)</span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true" tabindex="-1"></a>        g_preds <span class="op">=</span> torch.stack(g_preds)</span>
<span id="cb5-58"><a href="#cb5-58" aria-hidden="true" tabindex="-1"></a>        saved_g_preds[name] <span class="op">=</span> g_preds</span>
<span id="cb5-59"><a href="#cb5-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-60"><a href="#cb5-60" aria-hidden="true" tabindex="-1"></a>    <span class="co"># L1 baseline</span></span>
<span id="cb5-61"><a href="#cb5-61" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> name <span class="kw">in</span> saved_l1_preds:</span>
<span id="cb5-62"><a href="#cb5-62" aria-hidden="true" tabindex="-1"></a>        l1_preds <span class="op">=</span> saved_l1_preds[name]</span>
<span id="cb5-63"><a href="#cb5-63" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb5-64"><a href="#cb5-64" aria-hidden="true" tabindex="-1"></a>        l1_preds <span class="op">=</span> []</span>
<span id="cb5-65"><a href="#cb5-65" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> seed <span class="kw">in</span> <span class="bu">range</span>(n_runs):</span>
<span id="cb5-66"><a href="#cb5-66" aria-hidden="true" tabindex="-1"></a>            set_seed(seed)</span>
<span id="cb5-67"><a href="#cb5-67" aria-hidden="true" tabindex="-1"></a>            model <span class="op">=</span> ResMLP(input_dim, hidden_dim, depth(name), output_dim)</span>
<span id="cb5-68"><a href="#cb5-68" aria-hidden="true" tabindex="-1"></a>            X, Y <span class="op">=</span> train_data(ds_size(name), input_dim, <span class="bu">type</span> <span class="op">=</span> name, pre_additive_noise <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb5-69"><a href="#cb5-69" aria-hidden="true" tabindex="-1"></a>            dl <span class="op">=</span> torch.utils.data.DataLoader(<span class="bu">list</span>(<span class="bu">zip</span>(X, Y)), batch_size <span class="op">=</span> batch_size <span class="cf">if</span> batch_size <span class="cf">else</span> <span class="bu">len</span>(Y), shuffle <span class="op">=</span> <span class="va">True</span>, drop_last <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb5-70"><a href="#cb5-70" aria-hidden="true" tabindex="-1"></a>            l1_losses <span class="op">=</span> train_l1(model, dl, epochs <span class="op">=</span> epochs, verbose <span class="op">=</span> <span class="va">False</span>, lr <span class="op">=</span> lr(name))</span>
<span id="cb5-71"><a href="#cb5-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-72"><a href="#cb5-72" aria-hidden="true" tabindex="-1"></a>            <span class="cf">with</span> torch.no_grad():</span>
<span id="cb5-73"><a href="#cb5-73" aria-hidden="true" tabindex="-1"></a>                model.<span class="bu">eval</span>()</span>
<span id="cb5-74"><a href="#cb5-74" aria-hidden="true" tabindex="-1"></a>                l1_preds.append(model(t))</span>
<span id="cb5-75"><a href="#cb5-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-76"><a href="#cb5-76" aria-hidden="true" tabindex="-1"></a>        l1_preds <span class="op">=</span> torch.stack(l1_preds)</span>
<span id="cb5-77"><a href="#cb5-77" aria-hidden="true" tabindex="-1"></a>        saved_l1_preds[name] <span class="op">=</span> l1_preds</span>
<span id="cb5-78"><a href="#cb5-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-79"><a href="#cb5-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-80"><a href="#cb5-80" aria-hidden="true" tabindex="-1"></a>    X, Y <span class="op">=</span> train_data(ds_size(name), <span class="dv">1</span>, <span class="bu">type</span> <span class="op">=</span> name, pre_additive_noise <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb5-81"><a href="#cb5-81" aria-hidden="true" tabindex="-1"></a>    ax.scatter(X[:<span class="dv">5000</span>], Y[:<span class="dv">5000</span>], color <span class="op">=</span> <span class="st">'gray'</span>,  alpha <span class="op">=</span> <span class="fl">0.1</span>, s <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb5-82"><a href="#cb5-82" aria-hidden="true" tabindex="-1"></a>    ax.plot(t, g_star(t), label <span class="op">=</span> <span class="ss">f'True g'</span>, color <span class="op">=</span> <span class="st">'red'</span>)</span>
<span id="cb5-83"><a href="#cb5-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-84"><a href="#cb5-84" aria-hidden="true" tabindex="-1"></a>    ax.plot(t, l1_preds.mean(dim <span class="op">=</span> <span class="dv">0</span>), color <span class="op">=</span> <span class="st">'tab:blue'</span>, label <span class="op">=</span> <span class="st">'L1'</span>)</span>
<span id="cb5-85"><a href="#cb5-85" aria-hidden="true" tabindex="-1"></a>    ax.fill_between(t.flatten(),</span>
<span id="cb5-86"><a href="#cb5-86" aria-hidden="true" tabindex="-1"></a>        l1_preds.quantile(<span class="fl">0.10</span>, dim <span class="op">=</span> <span class="dv">0</span>).flatten(),</span>
<span id="cb5-87"><a href="#cb5-87" aria-hidden="true" tabindex="-1"></a>        l1_preds.quantile(<span class="fl">0.90</span>, dim <span class="op">=</span> <span class="dv">0</span>).flatten(),</span>
<span id="cb5-88"><a href="#cb5-88" aria-hidden="true" tabindex="-1"></a>        alpha <span class="op">=</span> <span class="fl">0.2</span>, color <span class="op">=</span> <span class="st">'tab:blue'</span></span>
<span id="cb5-89"><a href="#cb5-89" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb5-90"><a href="#cb5-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-91"><a href="#cb5-91" aria-hidden="true" tabindex="-1"></a>    ax.plot(t, g_preds.mean(dim <span class="op">=</span> <span class="dv">0</span>), color <span class="op">=</span> <span class="st">'tab:orange'</span>, label <span class="op">=</span> <span class="st">'engression'</span>)</span>
<span id="cb5-92"><a href="#cb5-92" aria-hidden="true" tabindex="-1"></a>    ax.fill_between(t.flatten(),</span>
<span id="cb5-93"><a href="#cb5-93" aria-hidden="true" tabindex="-1"></a>        g_preds.quantile(<span class="fl">0.10</span>, dim <span class="op">=</span> <span class="dv">0</span>).flatten(),</span>
<span id="cb5-94"><a href="#cb5-94" aria-hidden="true" tabindex="-1"></a>        g_preds.quantile(<span class="fl">0.90</span>, dim <span class="op">=</span> <span class="dv">0</span>).flatten(),</span>
<span id="cb5-95"><a href="#cb5-95" aria-hidden="true" tabindex="-1"></a>        alpha <span class="op">=</span> <span class="fl">0.2</span>, color <span class="op">=</span> <span class="st">'tab:orange'</span></span>
<span id="cb5-96"><a href="#cb5-96" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb5-97"><a href="#cb5-97" aria-hidden="true" tabindex="-1"></a>    ax.set_title(name)</span>
<span id="cb5-98"><a href="#cb5-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-99"><a href="#cb5-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-100"><a href="#cb5-100" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">0</span>, <span class="dv">0</span>].legend()</span>
<span id="cb5-101"><a href="#cb5-101" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">1</span>, <span class="dv">1</span>].set_ylim((<span class="op">-</span><span class="dv">2</span>, <span class="dv">5</span>))</span>
<span id="cb5-102"><a href="#cb5-102" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">1</span>, <span class="dv">0</span>].set_ylim((<span class="op">-</span><span class="dv">10</span>, <span class="dv">80</span>))</span>
<span id="cb5-103"><a href="#cb5-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-104"><a href="#cb5-104" aria-hidden="true" tabindex="-1"></a>f.tight_layout()</span>
<span id="cb5-105"><a href="#cb5-105" aria-hidden="true" tabindex="-1"></a>f.savefig(<span class="ss">f'images/figure4</span><span class="sc">{</span>extra<span class="sc">}</span><span class="ss">.png'</span>, dpi <span class="op">=</span> <span class="dv">300</span>, bbox_inches <span class="op">=</span> <span class="st">'tight'</span>)</span>
<span id="cb5-106"><a href="#cb5-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-107"><a href="#cb5-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-108"><a href="#cb5-108" aria-hidden="true" tabindex="-1"></a>torch.save(saved_g_preds, <span class="ss">f'logs/saved_g_preds</span><span class="sc">{</span>extra<span class="sc">}</span><span class="ss">.pt'</span>)</span>
<span id="cb5-109"><a href="#cb5-109" aria-hidden="true" tabindex="-1"></a>torch.save(saved_l1_preds, <span class="ss">f'logs/saved_l1_preds</span><span class="sc">{</span>extra<span class="sc">}</span><span class="ss">.pt'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="index_files/figure-html/cell-9-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Comparing engression and L1 regression’s extrapolation performance on synthetic data. Lines are predicted and true (red) conditional medians. Bands are 10-90 percentiles over 20 runs.</figcaption>
</figure>
</div>
</div>
</div>
<p>We find, as the authors do, that while both methods perform similarly in-domain, engression is much better extrapolating than L1 regression — at least for monotone and pre-additively generated data.</p>
<div class="callout callout-style-default callout-note no-icon callout-titled" title="What if the data is not pre-additive?">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
What if the data is not pre-additive?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>If we generate the synthetic data with post-additive noise instead, we see that engression looses its extrapolation capability — in accordance with the theory presented in the paper.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/figure4_post.png" class="img-fluid figure-img"></p>
<figcaption>Figure 4 repeated with data generated with post-additive noise instead.</figcaption>
</figure>
</div>
<p>A natural question, then, is whether one can diagnose whether the noise in a real dataset is pre- or post-additive. The paper doesn’t address this directly, but it’s an important question.</p>
</div>
</div>
</div>
<p>While reproducing Figure 4, we also noted a few practical details that matter more than expected. Despite the simplicity of the functions, the authors used between 50k and 100k samples, depending on the function, and relatively large networks. In our experiments, the cubic and logarithmic scenarios struggled with extrapolation until we added residual connections. Also, the noise dimension was set to 100, which seems surprisingly high but turned out to be important.</p>
</section>
<section id="hyperparameters" class="level2">
<h2 class="anchored" data-anchor-id="hyperparameters">Hyperparameters</h2>
<p>Engression introduces a couple of hyperparameters related to the noise vectors (<span class="math inline">\(\varepsilon\)</span>) sampled during training: namely, <span class="math inline">\(m\)</span>, the number of noise samples per example, and the distribution from which <span class="math inline">\(\varepsilon\)</span> is drawn. For example, in their synthetic data experiments, the authors set <span class="math inline">\(m = 2\)</span> and sample <span class="math inline">\(\varepsilon \sim \text{Unif}[0, 1]^{100}\)</span>.</p>
<p>The parameter <span class="math inline">\(m\)</span> controls an accuracy–compute trade-off. Increasing <span class="math inline">\(m\)</span> means we obtain a better Monte Carlo estimate of the population engression loss, and hence of the conditional distribution. However, this also increases compute: the cost of computing the loss grows linearly in <span class="math inline">\(m\)</span> for the first term but quadratically for the second, since we compute pairwise distances.</p>
<p>That said, since these are Monte Carlo estimates, we expect diminishing returns as <span class="math inline">\(m\)</span> increases. Another consolation point is that we can combat the quadratic cost by training for more epochs, at the expense of noisier gradients. This allows the model to revisit each example with different noise vectors, increasing the “effective” <span class="math inline">\(m\)</span>.</p>
<p>To test this, we vary <span class="math inline">\(m\)</span> on a toy example. We observe that, given enough data and epochs, all runs eventually reach similar loss values. However, models with higher <span class="math inline">\(m\)</span> converge faster and more stably. Perhaps not surprisingly, the conditional median tends to be learned first, with the tails filling in later.</p>
<!-- TODO find reference pointers for last sentence -->
<div id="b2df4995" class="cell" data-execution_count="10">
<details class="code-fold">
<summary>Vary <span class="math inline">\(m\)</span> on toy data</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">5_000</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> torch.rand(n) <span class="op">*</span> <span class="dv">2</span> <span class="op">-</span> <span class="dv">1</span>               <span class="co"># X ~ Unif(-1, 1)</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> X <span class="op">+</span> (torch.randn_like(X) <span class="op">*</span> <span class="dv">1</span>)       <span class="co"># Y|X ~ N(X, 1)</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>X, Y <span class="op">=</span> X[:, <span class="va">None</span>], Y[:, <span class="va">None</span>]</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> torch.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">25</span>)[:, <span class="va">None</span>]</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="co"># True Y|X quantiles if we assume Y|X ~ N(X, 1)</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>qs <span class="op">=</span> [<span class="fl">0.1</span>, <span class="fl">0.5</span>, <span class="fl">0.9</span>]</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>true_quantiles <span class="op">=</span> {q: t <span class="op">+</span> norm.ppf(q) <span class="cf">for</span> q <span class="kw">in</span> qs}</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Estimated Y|X quantiles</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="co"># [m_train, quantile level] = list of [est quantile], one per batch</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>quantiles <span class="op">=</span> {}</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>input_dim, hidden_dim, output_dim <span class="op">=</span> <span class="dv">1</span>, <span class="dv">64</span>, <span class="dv">1</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>noise_dim <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>mini_batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>m_pred <span class="op">=</span> <span class="dv">512</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a><span class="co"># noise_scale = 1</span></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>stats <span class="op">=</span> defaultdict(<span class="bu">list</span>)</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>m_trains <span class="op">=</span> [<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">8</span>, <span class="dv">16</span>]</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>noise_scales <span class="op">=</span> [<span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>]</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> m_train, noise_scale <span class="kw">in</span> product(m_trains, noise_scales):</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>    set_seed(<span class="dv">69</span>)</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>    dl <span class="op">=</span> torch.utils.data.DataLoader(<span class="bu">list</span>(<span class="bu">zip</span>(X, Y)), batch_size <span class="op">=</span> mini_batch_size, shuffle <span class="op">=</span> <span class="va">False</span>)</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> ResMLP(input_dim <span class="op">=</span> input_dim <span class="op">+</span> noise_dim, hidden_dim <span class="op">=</span> hidden_dim, num_residual_blocks <span class="op">=</span> <span class="dv">2</span>, output_dim <span class="op">=</span> output_dim)</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>    g <span class="op">=</span> gConcatenate(model, noise_dim <span class="op">=</span> noise_dim, scale <span class="op">=</span> noise_scale)</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> torch.optim.Adam(g.parameters(), lr <span class="op">=</span> <span class="fl">1e-4</span>)</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> x, y <span class="kw">in</span> dl:</span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a>            g.zero_grad()</span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Generate m samples from the model</span></span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a>            preds <span class="op">=</span> torch.stack([g(x) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(m_train)], dim <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a>            loss, loss_1, loss_2 <span class="op">=</span> engression_loss(y, preds, return_terms <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a>            optimizer.step()</span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a>            <span class="cf">with</span> torch.no_grad():</span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a>                q_losses <span class="op">=</span> []</span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a>                preds <span class="op">=</span> torch.stack([g(t) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(m_pred)], dim <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> q <span class="kw">in</span> qs:</span>
<span id="cb6-54"><a href="#cb6-54" aria-hidden="true" tabindex="-1"></a>                    pred_quantile <span class="op">=</span> torch.quantile(preds, q <span class="op">=</span> q, dim <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb6-55"><a href="#cb6-55" aria-hidden="true" tabindex="-1"></a>                    quantiles[m_train, noise_scale, q] <span class="op">=</span> quantiles.get((m_train, noise_scale, q), []) <span class="op">+</span> [pred_quantile]</span>
<span id="cb6-56"><a href="#cb6-56" aria-hidden="true" tabindex="-1"></a>                    </span>
<span id="cb6-57"><a href="#cb6-57" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> q <span class="kw">in</span> true_quantiles:</span>
<span id="cb6-58"><a href="#cb6-58" aria-hidden="true" tabindex="-1"></a>                        stats[m_train, noise_scale, q].append(</span>
<span id="cb6-59"><a href="#cb6-59" aria-hidden="true" tabindex="-1"></a>                            (true_quantiles[q] <span class="op">-</span> pred_quantile.mean(dim <span class="op">=</span> <span class="dv">0</span>)).flatten().<span class="bu">abs</span>().mean().item()</span>
<span id="cb6-60"><a href="#cb6-60" aria-hidden="true" tabindex="-1"></a>                        )</span>
<span id="cb6-61"><a href="#cb6-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-62"><a href="#cb6-62" aria-hidden="true" tabindex="-1"></a>                stats[m_train, noise_scale, <span class="st">'loss'</span>].append(loss.item())</span>
<span id="cb6-63"><a href="#cb6-63" aria-hidden="true" tabindex="-1"></a>                stats[m_train, noise_scale, <span class="st">'loss_1'</span>].append(loss_1.item())</span>
<span id="cb6-64"><a href="#cb6-64" aria-hidden="true" tabindex="-1"></a>                stats[m_train, noise_scale, <span class="st">'loss_2'</span>].append(loss_2.item())</span>
<span id="cb6-65"><a href="#cb6-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-66"><a href="#cb6-66" aria-hidden="true" tabindex="-1"></a>torch.save((quantiles, stats), <span class="st">'logs/m_noise_scale.pt'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="fd9269f9" class="cell" data-execution_count="9">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="index_files/figure-html/cell-14-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Effects of varying <span class="math inline">\(m\)</span> on a simple example. Increasing <span class="math inline">\(m\)</span> yields more stable losses and faster reduction in the error estimating <span class="math inline">\(Y|X\)</span> quantiles.</figcaption>
</figure>
</div>
</div>
</div>
<p>While the only formal requirement on the noise distribution is that <span class="math inline">\(\varepsilon\)</span> be independent of <span class="math inline">\(x\)</span>, its specific choice can have important practical implications. For instance, the authors show that increasing the standard deviation of <span class="math inline">\(\varepsilon\)</span> can improve extrapolation.</p>
<p>In our toy setup, the effect of <span class="math inline">\(\sigma_\varepsilon\)</span> resembles a smoothing or locality parameter: large values encourage distant <span class="math inline">\(x\)</span>’s to map to the same <span class="math inline">\(y\)</span>, producing smoother estimates. Imagining one-dimensional <span class="math inline">\(\varepsilon\)</span> and <span class="math inline">\(g(x + \varepsilon)\)</span> (instead of concatenating) should help with this intuition. While larger values benefit extrapolation in other settings (as shown in the paper), here we see that it could hurt in-domain.</p>
<div id="917c82b5" class="cell">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="index_files/figure-html/cell-15-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Conditional medians learned after training with gaussian noise with different <span class="math inline">\(\sigma_\varepsilon\)</span>, with <span class="math inline">\(m=2\)</span>.</figcaption>
</figure>
</div>
</div>
</div>
<p>Above, we fixed <span class="math inline">\(m\)</span> and varied <span class="math inline">\(\sigma_\varepsilon\)</span>. But in general, <span class="math inline">\(m\)</span>, <span class="math inline">\(\sigma_\varepsilon\)</span>, and the dimensionality of <span class="math inline">\(\varepsilon\)</span> appear to be interdependent. For example (speculating), increasing <span class="math inline">\(\sigma_\varepsilon\)</span> might require lower noise dimensionality to avoid exploding variance, or perhaps lower <span class="math inline">\(m\)</span> values suffer more under high-variance noise due to noisier gradient estimates.</p>
<p>It would be interesting to study the interplay between them. Tuning them jointly will likely be important for practical applications.</p>
</section>
<section id="real-data" class="level2">
<h2 class="anchored" data-anchor-id="real-data">Real data</h2>
<p>So far, we’ve focused on toy problems and synthetic experiments, but the authors of the engression paper also evaluate the method on real datasets in Section 4. They benchmark engression on both univariate and multivariate tasks — including point prediction, interval estimation, and full distributional modeling — and compare it against standard approaches like L2 and L1 regression, as well as <a href="https://www.jmlr.org/papers/volume7/meinshausen06a/meinshausen06a.pdf">quantile regression forests</a>. Engression consistently outperforms these baselines, especially when extrapolating.</p>
<p>In their experiments, the authors use a MLP trained directly on the tabular inputs. But it’s <a href="https://arxiv.org/abs/2207.08815">well known</a> that deep nets are not particularly strong on tabular data (although this might be <a href="https://www.nature.com/articles/s41586-024-08328-6">changing</a>). That said, there’s nothing inherent in engression that requires us to use a neural network end-to-end. We could just as easily treat the engression-trained MLP as a modular head and stack it on top of any strong tabular model.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/stacking_engression.svg" class="img-fluid figure-img"></p>
<figcaption>We can stack engression on top of any base model</figcaption>
</figure>
</div>
<p>I tried <a href="_temperature.ipynb">this</a> idea on a probabilistic forecasting <a href="https://www.kaggle.com/competitions/probabilistic-forecasting-i-temperature/overview">competition</a>, where the goal was to predict a set of conditional quantiles (0.05 through 0.95). The winning submission had used <a href="https://catboost.ai/">catboost</a> with a multi-quantile loss, followed by <a href="https://en.wikipedia.org/wiki/Conformal_prediction">conformal prediction</a>. Keeping the base model and replacing the conformalization step with a small engression-trained MLP improved on the winning model’s <a href="https://en.wikipedia.org/wiki/Scoring_rule#Conditional_continuous_ranked_probability_score">CRPS score</a> by about 0.04 on the private test set<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> – without tuning any of the engression hyperparameters. I thought this was impressive, as conformalization is the go-to calibration method right now. Of course, if you wanted finite-sample guarantees, you could still conformalize on top.</p>
<p>The broader point is that the pre-additive noise assumption underlying engression appears to hold up on real-world datasets — and that stacking engression as a flexible head could make it a practical upgrade to existing tabular pipelines.</p>
</section>
<section id="final-thoughts" class="level2">
<h2 class="anchored" data-anchor-id="final-thoughts">Final thoughts</h2>
<p>Engression’s main strength lies in its simplicity and flexibility. It requires no parametric assumptions on the output distribution, no likelihood computations, no adversarial training, and no architectural constraints like invertibility. It scales naturally to multivariate <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, and at test time, sampling is fast and easily parallelizable. These properties make it particularly appealing for tasks like forecasting, simulation, or structured prediction, where calibrated uncertainty is important but explicit density evaluation is not.</p>
<p>That said, engression comes with tradeoffs. Because it models distributions implicitly — without yielding closed-form densities — it’s less suitable for inference tasks that rely on likelihoods. Its one-shot sampling may also struggle with complex, multimodal distributions, as the energy score tends to cover modes rather than <a href="https://sander.ai/2020/03/24/audio-generation.html#mode-covering-vs-mode-seeking-behaviour">seek them</a>. In such cases, methods like diffusion models or normalizing flows might offer better performance, albeit at higher computational and implementation cost.</p>
<p>Encouragingly, the authors have begun to explore extensions. A recent <a href="https://arxiv.org/abs/2502.13747v1">paper</a> proposes a multi-step version of engression that improves performance on challenging tasks. Another work introduces <a href="https://arxiv.org/abs/2404.13649">distributional autoencoders</a>, combining engression with dimensionality reduction.</p>
<p>Overall, engression is a clever and lightweight approach to distributional regression. I’m excited to see where future research and applications take it. The <a href="https://arxiv.org/abs/2307.00835">paper</a> is very readable, and the authors have released their code <a href="https://github.com/xwshen51/engression/">here</a> if you want to play with it. I’m also learning about python packaging — here’s a <a href="https://github.com/emiliocantuc/engression-pytorch">small one</a> with the loss and a few wrappers for convenience.</p>
<hr>
<p>Thanks for reading! If you spot any errors, or have comments or suggestions, feel free to reach out.</p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>For context, the difference between first and second place was 0.06.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p><a href="../../../">Emilio Cantu-Cervini</a></p>
<div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/emiliocantuc/emiliocantuc.github.io/blob/main/posts/papers/engression/index.ipynb" class="toc-action"><i class="bi bi-github"></i>View source</a></li></ul></div></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>